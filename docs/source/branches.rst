Branches
========

이 프로젝트는 하나의 저장소 안에서 여러 브랜치로 기능을 나누어 개발합니다.

현재 사용 중인 브랜치는 다음과 같습니다.

text 브랜치
-----------

- 브랜치 이름: ``text``
- 역할: 욕설필터
- 예시 내용:  
  - 텍스트 분류
  - 토큰화, 전처리
  - 간단한 NLP 유틸리티 함수 등

imag 브랜치
-----------

- 브랜치 이름: ``imag``
- 역할: 이미지 처리 관련 예제를 포함합니다.
- 예시 내용:  
  - 이미지 읽기/쓰기
  - 기본 필터 적용
  - OpenCV 또는 PIL 기반 변환 등

audio branch
==========

- 브랜치 이름: `audio`
- 역할: 동영상(MP4)에서 무음 구간을 자동으로 감지·제거하여 핵심 오디오만 요약·추출한다.
- 주요 프로세스:
  - `Input`: 비디오 파일 입력 (`data/input`)
  - `Convert`: 오디오 트랙 추출 및 포맷 변환 (MP4 → WAV)
  - `Visualize`: 처리 전 오디오 파형(Waveform) 시각화 및 저장
  - `Process`: RMS 에너지 기반 무음 감지 및 노이즈 필터링
  - `Output`: 유효 음성 구간 병합 및 최종 파일 저장

- 모듈 구성:
  - `main.py` (Controller)
    - 전체 파이프라인 실행 및 흐름 제어
    - 예외 처리 및 파일 경로 관리
  - `convert.py` (Converter)
    - `MoviePy`를 활용한 미디어 포맷 변환
    - 비디오 내 오디오 스트림 분리
  - `process.py` (Core Logic)
    - `Librosa` 기반 오디오 신호 분석
    - 데시벨(dB) 기준 무음 구간 절삭 알고리즘
    - 데이터 정규화(Normalization) 및 시각화
  - `file.py` (I/O Handler)
    - `SoundFile`을 이용한 고속 오디오 데이터 입출력
    - 결과물 저장을 위한 디렉토리 자동 생성

- 사용 기술 스택:
  - `Language`: Python
  - `Libraries`: MoviePy, Librosa, NumPy, Matplotlib, SoundFile

- 기대 효과:
  - 원본 영상 대비 재생 시간 단축
  - 편집 없는 핵심 구간 자동 요약

video branch
==========================

- 브랜치 이름: `video`
- 하위 폴더: `video_hyenseok`
- 역할: 동영상(MP4)에서 사람 얼굴을 자동으로 감지하여 해당 영역만 강하게 블러 처리하는 필터를 구현한다.
- 주요 프로세스:
  - `Input`: 비디오 파일 경로를 지정하여 `cv2.VideoCapture`로 로드  
    (예: `video_yeonbeom/video.mp4`, `video_hyenseok/49초.mp4` 등)
  - `Inspect`: 첫 프레임을 읽어 원본 해상도(`width`, `height`)와 FPS 정보를 출력
  - `Writer`: 원본 영상의 FPS·해상도에 맞춰 `cv2.VideoWriter`로 출력 파일(`output_blur.mp4`) 생성
  - `Detect`: MediaPipe `FaceDetection`으로 매 프레임 얼굴 위치(bounding box) 추정
  - `Pad & Clamp`: 얼굴 박스를 일정 비율(15%) 확장하고, 프레임 경계를 넘지 않도록 보정
  - `Blur`: 검출된 각 얼굴 영역에 대해 가우시안 블러(`GaussianBlur`) 적용
  - `Display`: 화면에서는 보기 편하도록 가로 1280px 기준으로 리사이즈하여 출력
  - `Output`: 블러 처리된 프레임을 동영상으로 저장하고, ESC 키 입력 시 종료

- 스크립트 구성 (`video_hyenseok/face_detection.py`):
  - **전역 설정**
    - `mp_face_detection = mp.solutions.face_detection`
    - `mp_drawing = mp.solutions.drawing_utils`
    - 입력 비디오 경로, 출력 비디오 경로를 코드 상단에서 직접 지정
  - **비디오 캡처 초기화**
    - `cv2.VideoCapture(입력경로)` 로 동영상 오픈
    - 첫 프레임을 읽어 실패 시 메시지 출력 후 종료
    - 프레임 해상도, FPS를 콘솔에 출력하고, 프레임 포인터를 0으로 되돌림
  - **VideoWriter 설정**
    - FourCC 코덱: `mp4v`
    - FPS: 입력 영상과 동일 (`cap.get(cv2.CAP_PROP_FPS)`)
    - 해상도: 입력 영상과 동일 (`CAP_PROP_FRAME_WIDTH`, `CAP_PROP_FRAME_HEIGHT`)
    - 출력 파일: `video_hyenseok/output_blur.mp4`
  - **얼굴 감지 루프**
    - `with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.1)`  
      블록 안에서 매 프레임 처리
    - `alpha`, `beta` 파라미터로 영상 대비·밝기 조절 (`cv2.convertScaleAbs`)
    - BGR → RGB 변환 후 `face_detection.process()`로 얼굴 검출
  - **Bounding Box 계산**
    - `relative_bounding_box`(비율값)를 사용해 픽셀 단위 좌표 `(x1, y1, x2, y2)` 계산
    - 가로·세로 길이의 15%를 pad로 추가하여 얼굴 주변까지 블러 처리
    - `max`, `min`으로 영상 경계 내로 좌표 클램핑
  - **블러 적용**
    - 각 얼굴 영역(`image[y1:y2, x1:x2]`)에 대해
      - 비어 있는 영역은 건너뜀
      - `cv2.GaussianBlur(face_region, (71, 71), 35)`로 강한 블러 적용
    - 블러링 된 결과를 원본 프레임의 해당 위치에 다시 덮어씀
  - **시각화 및 종료 처리**
    - 표시용 프레임은 가로 1280px로 리사이즈하여 `cv2.imshow("Face Blur", image)`로 출력
    - `cv2.waitKey(5)`에서 ESC(`27`) 입력 시 루프 종료
    - 마지막에 `cap.release()`, `out.release()`, `cv2.destroyAllWindows()`로 자원 정리

- 사용 기술 스택:
  - `Language`: Python
  - `Libraries`: OpenCV (`cv2`), MediaPipe (`mp.solutions.face_detection`)
  - 기타: 가우시안 블러를 활용한 프라이버시 보호 처리

- 기대 효과:
  - 얼굴 부분만 자동 블러 처리하여 모자이크 효과 구현
  - 해상도·FPS를 원본과 동일하게 유지하여 후속 편집 프로그램과의 호환성 확보

main 브랜치
-----------

- 브랜치 이름: ``main``
- 역할: 프로젝트 공통 리소스 및 문서(Sphinx), 예제 모듈(lumache.py)을 포함합니다.
- Read the Docs에서 빌드되는 기본 브랜치입니다.
